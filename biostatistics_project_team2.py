# -*- coding: utf-8 -*-
"""Biostatistics_Project_Team2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eBmvPE6NXl-fBaaLiE4Wz3102N7CtuPt

# Select the dataset
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from google.colab import files
import io

data = files.upload()

data=pd.read_csv(io.StringIO(data["Breast_cancer_data.csv"].decode('utf-8')))

data.head(10)
'''all the features are quantitative, meaning that they represent
numerical values that can be measured and compared.
The diagnosis variable of the dataset is binary, with a value of 0 representing
 a benign tumor and a value of 1 representing a malignant tumor.'''

"""# Cleaning the data and Removing the outliers"""

#Cleaning the data
print(data.shape)
data = data.dropna()  #Return a new Data Frame with no empty cells
data.drop_duplicates(inplace = True)   #Removing Duplicates
print(data.shape)

''''We note that the size of the data has not changed,
which means that there was no missing values or duplicates'''

# Removing the outliers from mean_radius
for feature in data.columns:
    up_limit = data[feature].mean() + 3 * data[feature].std()
    low_limit = data[feature].mean() - 3 * data[feature].std()
    outliers = data[(data[feature] > up_limit) | (data[feature] < low_limit)]
    data = data[(data[feature] < up_limit) & (data[feature] > low_limit)]
print("Shape after removing outliers from mean_radius:",data.shape)

"""# Measures of Central Tendency"""

'''There are three common measures of central
tendency: mean, median, and mode.'''
#  mean
means = data.mean()

#  median
medians = data.median()

#  mode
modes = data.mode().iloc[0]

# Print the means, medians, and modes for each feature in our dataset
for column in data.columns:
    print("column:", column)
    print("Mean:", means[column])
    print("Median:", medians[column])
    print("Mode:", modes[column])

"""# Measures of Dispersion"""

'''There are several measures of dispersion
including range, variance, and standard deviation'''

# Calculate the range for each feature
ranges = data.max() - data.min()

# Calculate the standard deviation for each feature
stds = data.std()

# Calculate the variance for each feature
vars = data.var()

# Print the ranges, std, and variances for each feature
for column in data.columns:
    print("column:", column)
    print("Range:", ranges[column])
    print("Standard Deviation:", stds[column])
    print("Variance:", vars[column])

#Standardize the features using our calculations for the descriptive statistics.

data['mean_radius']=(data['mean_radius'] - data['mean_radius'].mean())/data['mean_radius'].std()
data['mean_texture']=(data['mean_texture'] - data['mean_texture'].mean())/data['mean_texture'].std()
data['mean_perimeter']=(data['mean_perimeter'] - data['mean_perimeter'].mean())/data['mean_perimeter'].std()
data['mean_area']=(data['mean_area'] - data['mean_area'].mean())/data['mean_area'].std()
data['mean_smoothness']=(data['mean_smoothness'] - data['mean_smoothness'].mean())/data['mean_smoothness'].std()

data.describe()[['mean_radius', 'mean_texture','mean_perimeter','mean_area','mean_smoothness','diagnosis']]

"""# Split the data randomly into 2 partitions with a 80%-20% proportion"""

#Split the data randomly into 2 partitions with a 80%-20% proportion
from sklearn.model_selection import  train_test_split
train, test = train_test_split(data, test_size=0.2)
print("Train shape:", train.shape)
print("Test shape:", test.shape)

#For each feature/column in the training , Plot the histogram/distribution
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(2, 3, figsize=(12, 8), sharey=True)

sns.histplot(train, ax=axes[0, 0], x="mean_radius", kde=True)
sns.histplot(train, ax=axes[0, 1], x="mean_texture", kde=True)
sns.histplot(train, ax=axes[0, 2], x="mean_perimeter" , kde=True)
sns.histplot(train, ax=axes[1, 0], x="mean_area", kde=True)
sns.histplot(train, ax=axes[1, 1], x="mean_smoothness", kde=True)

plt.tight_layout()
plt.show()
#The histograms appears to be roughly symmetric and bell-shaped, which suggests that the data may be normally distributed

"""# **Hypothesis Test**"""

from scipy.stats import shapiro

alpha = 0.05  # Set the significance level

for feature in data.columns:
    statistic, p_value = shapiro(data[feature])
    if ((p_value > alpha) | (p_value < 1 - alpha)):
        print(f"The {feature} feature/column is normally distributed (fail to reject H0)")
    else:
        print(f"The {feature} feature/column is not normally distributed (reject H0)")

"""# Plot Conditional Distributions"""

import seaborn as sns
import matplotlib.pyplot as plt

# Plot conditional distributions for each feature on the target class
fig, axes = plt.subplots(2, 3, figsize=(12, 8))

sns.histplot(data=train, x="mean_radius", hue="diagnosis", kde=True, ax=axes[0, 0])
sns.histplot(data=train, x="mean_texture", hue="diagnosis", kde=True, ax=axes[0, 1])
sns.histplot(data=train, x="mean_perimeter", hue="diagnosis", kde=True, ax=axes[0, 2])
sns.histplot(data=train, x="mean_area", hue="diagnosis", kde=True, ax=axes[1, 0])
sns.histplot(data=train, x="mean_smoothness", hue="diagnosis", kde=True, ax=axes[1, 1])

# Adjust the layout
#function call adjusts the spacing between subplots to improve their readability.
plt.tight_layout()

# Display the plots
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

def plot_conditional_distributions(train, feature):
    """
    Plot conditional distributions of a feature based on the diagnosis (label).

    Parameters:
    - data: DataFrame containing the data
    - feature: Name of the feature to plot
    """
    g = sns.FacetGrid(data, col="diagnosis", height=4)
    g.map(sns.histplot, feature, kde=True)
    plt.show()

# List of features to plot
features = ["mean_radius", "mean_texture", "mean_perimeter", "mean_area", "mean_smoothness"]

# Plot conditional distributions for each feature
for feature in features:
    plot_conditional_distributions(train, feature)

"""# Naive Bayes Classifier formula can be written based on Bayes theorem as:

# $P(y|x_1, \ldots, x_j) = \frac{P(x_1, \ldots, x_j|y) \cdot P(y)}{P(x_1, \ldots, x_j)}$

### where $y$ represents the dependent variable, and $x_1, \ldots, x_j$ represent the independent features.

### $-P(y|x_1, \ldots, x_j)$ is the posterior probability of $y$ given the features $x_1, \ldots, x_j$.

### $-P(x_1, \ldots, x_j|y)$ is the likelihood of observing the features $x_1, \ldots, x_j$ given that the class is $y$.

### $-P(y)$ is the prior probability of $y$.

### $-P(x_1, \ldots, x_j)$ is the marginal probability of observing the features $x_1, \ldots, x_j$.
"""

# Calculate P(Y=y) for all possible y
'''Calculate Prior Probabilities for Each Class
This function calculates the prior probabilities for each class (Y=y) in the given dataset (df) based on a target variable (Y).
It takes two parameters: df - the dataset, and Y - the name of the target variable.
The function first identifies all unique classes in the target variable and sorts them in ascending order.
It then iterates over each class and calculates the prior probability by dividing the count of instances with that class by the total number of instances in the dataset.
The prior probabilities are stored in a list and returned by the function.'''


def calculate_prior(df, Y):
    classes = sorted(list(df[Y].unique()))
    prior = []
    for i in classes:
        prior.append(len(df[df[Y]==i])/len(df))
    return prior

# P(X=x|Y=y) using Gaussian dist.
""" Calculate Likelihood Probability for Gaussian Distribution

 This function calculates the likelihood probability for a given feature value (feat_val) of a specific feature (feat_name) in a Gaussian distribution, conditioned on a target variable (Y) having a specific label.
 It takes five parameters: df - the dataset, feat_name - the name of the feature, feat_val - the value of the feature, Y - the name of the target variable, and label - the specific label of the target variable.
 The function first retrieves the column names of the dataset.
 It then filters the dataset to include only the instances where the target variable has the specified label.
 Next, it calculates the mean and standard deviation of the feature within the filtered dataset.
 Using these parameters, the function calculates the likelihood probability using the Gaussian probability density function formula."""

def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):
    feat = list(df.columns)
    df = df[df[Y]==label]
    mean, std = df[feat_name].mean(), df[feat_name].std()
    p_x_given_y = (1 / (np.sqrt(2 * np.pi) * std)) *  np.exp(-((feat_val-mean)**2 / (2 * std**2 )))
    return p_x_given_y

#P(X=x1|Y=y)P(X=x2|Y=y)...P(X=xn|Y=y)
"""# Naive Bayes Classifier with Gaussian Distribution

 This function implements the Naive Bayes classifier using the Gaussian distribution assumption for the feature variables.
 It takes three parameters: df - the dataset, X - the input features to classify, and Y - the target variable.
 The function starts by extracting the feature names from the dataset.
 It then calculates the prior probabilities for each class using the calculate_prior() function.
 Next, it iterates over each input sample (x) and performs the following steps:
 - Calculates the likelihood probabilities for each class using the calculate_likelihood_gaussian() function.
 - Initializes a list to store the posterior probabilities (numerator only).
 - For each class, multiplies the corresponding likelihood probabilities by the prior probability to calculate the posterior probability numerator.
 - Appends the predicted class (index with the maximum posterior probability) to the list of predicted labels (Y_pred)."""

def naive_bayes_gaussian(df, X, Y):
    # get feature names
    features = list(df.columns)[:-1]

    # calculate prior
    prior = calculate_prior(df, Y)

    Y_pred = []
    # loop over every data sample
    for x in X:
        # calculate likelihood
        labels = sorted(list(df[Y].unique()))
        likelihood = [1]*len(labels)
        for j in range(len(labels)):
            for i in range(len(features)):
                likelihood[j] *= calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])

        # calculate posterior probability (numerator only)
        post_prob = [1]*len(labels)
        for j in range(len(labels)):
            post_prob[j] = likelihood[j] * prior[j]

        Y_pred.append(np.argmax(post_prob))

    return np.array(Y_pred)

# Test Gaussian model
from sklearn.model_selection import train_test_split
train, test = train_test_split(data, test_size=.2, random_state=41)

X_test = test.iloc[:,:-1].values
Y_test = test.iloc[:,-1].values
Y_pred = naive_bayes_gaussian(train, X=X_test, Y="diagnosis")

from sklearn.metrics import confusion_matrix, f1_score
print(confusion_matrix(Y_test, Y_pred))
print(f1_score(Y_test, Y_pred))

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)

from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
y_pred = gnb.fit(X_train, y_train).predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

#comparison
#here we can deduce a difference of 2.47% difference between the 2 accuracies in favor of the last one